# -*- coding: utf-8 -*-
"""homework_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jYByWP05T9QtyjZ2IB2j6tei1UZy8Khj

## Dataset download
"""

# !gdown --id '1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy' --output food-11.zip

# This may take some time.
# !unzip -q food-11.zip

"""## Import packages
Tutorial says **torchvision** is recommended.
"""

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image

# There are some useful packages for semi-supervised learning like Subset and etc.
from torch.utils.data import ConcatDataset, DataLoader, Subset
from torchvision.datasets import DatasetFolder

# This is for the process bar.
from tqdm.auto import tqdm

"""## Dataset, Data Loader, and Transforms
Not every augmention is useful for a certain task. In food images classification task, I think rotation might be useful.
"""

train_tfm = transforms.Compose([
    transforms.Resize((128, 128)),
    # Some transforms should be added here.

    transforms.RandomResizedCrop(128, scale=(0.6, 1.0), ratio=(3/4, 4/3), antialias=True),
    transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally
    transforms.RandomRotation(15),     # Randomly rotate the image by up to 15 degrees
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.15, hue=0.03), # Randomly change the brightness, contrast, saturation, and hue
    transforms.ToTensor()
])

# Augmentations in testing and validation are not needed.
# Only resize is needed.
test_tfm = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

batch_size = 128

# Tell torchvision how to read the data.
train_set = DatasetFolder("food-11/training/labeled", loader=lambda x: Image.open(x), extensions='jpg', transform=train_tfm)
valid_set = DatasetFolder("food-11/validation", loader=lambda x: Image.open(x), extensions="jpg", transform=test_tfm)

# This is for semi-supervised learning.
# *****************************************
unlabeled_set = DatasetFolder("food-11/training/unlabeled", loader=lambda x: Image.open(x), extensions="jpg", transform=train_tfm)
# *****************************************
test_set = DatasetFolder("food-11/testing", loader=lambda x: Image.open(x), extensions="jpg", transform=test_tfm)

# Construct data loaders.
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)

"""## Model Init
Tutorial writes basic model architecture. But some improvements can be done and are waiting for exploring.
"""

class Classifier(nn.Module):
  def __init__(self):
    super(Classifier, self).__init__()
    self.cnn_layers = nn.Sequential(
        nn.Conv2d(3, 64, 3, 1, 1),
        nn.BatchNorm2d(64),
        nn.ReLU(),
        nn.MaxPool2d(2, 2, 0),

        nn.Conv2d(64, 128, 3, 1, 1),
        nn.BatchNorm2d(128),
        nn.ReLU(),
        nn.MaxPool2d(2, 2, 0),

        nn.Conv2d(128, 256, 3, 1, 1),
        nn.BatchNorm2d(256),
        nn.ReLU(),
        nn.MaxPool2d(4, 4, 0),
    )
    self.fc_layers = nn.Sequential(
        nn.Linear(256 * 8 * 8, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 11)
    )

  def forward(self, x):
    x = self.cnn_layers(x)
    x = x.flatten(1)
    x = self.fc_layers(x)
    return x

"""## Training
Below contains a function about semi-supeprvised learning.
"""

# Some possible smallest modifications are added.
# Fixed pseudo-labeling function
class PseudoLabelDataset(torch.utils.data.Dataset):
  """Dataset wrapper that returns pseudo labels for selected indices."""

  def __init__(self, base_dataset, indices, labels):
    self.base_dataset = base_dataset
    self.indices = indices
    self.labels = labels

  def __len__(self):
    return len(self.indices)

  def __getitem__(self, idx):
    img, _ = self.base_dataset[self.indices[idx]]
    return img, self.labels[idx]


def get_pseudo_labels(dataset, model, threshold=0.65, used_indices=None):
  device = "cuda" if torch.cuda.is_available() else "cpu"
  print("Generating pseudo labels for unlabeled data...")
  data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

  model.eval()

  softmax = nn.Softmax(dim=-1)

  selected_indices = []
  selected_labels = []
  seen = used_indices if used_indices is not None else set()
  global_idx = 0

  for imgs, _ in tqdm(data_loader):
    with torch.no_grad():
      logits = model(imgs.to(device))

    probs = softmax(logits)
    max_probs, preds = torch.max(probs, dim=1)

    for i in range(len(imgs)):
      if global_idx in seen:
        global_idx += 1
        continue

      if max_probs[i].item() >= threshold:
        selected_indices.append(global_idx)
        selected_labels.append(preds[i].item())
        seen.add(global_idx)

      global_idx += 1

  model.train()

  print(f"Selected {len(selected_indices)} pseudo-labeled samples with threshold {threshold}.")
  return PseudoLabelDataset(dataset, selected_indices, selected_labels), seen

# This block is about training parameters setting.
device = "cuda" if torch.cuda.is_available() else "cpu"

model = Classifier().to(device)
model.device = device

criterion = nn.CrossEntropyLoss()

# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.
optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-5)
# Add learning rate scheduler
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80, eta_min=1e-6)

n_epochs = 100

# Whether to do semi-supervised learning.
do_semi = True
semi_update_interval = 10
semi_threshold = 0.75
pseudo_datasets = []
used_unlabeled_indices = set()

# Early stopping parameters
patience = 10  # How many epochs to wait after last improvement.
best_acc = 0.0
epochs_no_improve = 0

for epoch in range(n_epochs):
    # ---------- Semi-supervised update ----------
    if do_semi and (epoch + 1) % semi_update_interval == 0 and len(used_unlabeled_indices) < len(unlabeled_set):
        pseudo_set, used_unlabeled_indices = get_pseudo_labels(
            unlabeled_set, model, threshold=semi_threshold, used_indices=used_unlabeled_indices
        )

        if len(pseudo_set) > 0:
            print(f"Adding {len(pseudo_set)} pseudo-labeled samples to the training set.")
            pseudo_datasets.append(pseudo_set)
            concat_dataset = ConcatDataset([train_set] + pseudo_datasets)
            train_loader = DataLoader(
                concat_dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=0,
                pin_memory=True,
            )
        else:
            print("No new pseudo-labeled samples were selected in this interval.")
    elif do_semi and (epoch + 1) % semi_update_interval == 0:
        print("All unlabeled samples have already been assigned pseudo labels.")

    # ---------- Training ----------
    # Make sure the model is in train mode before training.
    model.train()

    train_loss = []
    train_accs = []

    for batch in tqdm(train_loader):
        imgs, labels = batch

        logits = model(imgs.to(device))

        loss = criterion(logits, labels.to(device))

        optimizer.zero_grad()

        loss.backward()

        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)

        optimizer.step()

        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        train_loss.append(loss.item())
        train_accs.append(acc.item())

    # The average loss and accuracy of the training set is the average of the recorded values.
    train_loss = sum(train_loss) / max(len(train_loss), 1)
    train_acc = sum(train_accs) / max(len(train_accs), 1)

    # Print the information.
    print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")

    # ---------- Validation ----------
    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.
    model.eval()

    # These are used to record information in validation.
    valid_loss = []
    valid_accs = []

    # Iterate the validation set by batches.
    for batch in tqdm(valid_loader):

        imgs, labels = batch

        with torch.no_grad():
          logits = model(imgs.to(device))

        loss = criterion(logits, labels.to(device))

        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        valid_loss.append(loss.item())
        valid_accs.append(acc.item())

    valid_loss = sum(valid_loss) / len(valid_loss)
    valid_acc = sum(valid_accs) / len(valid_accs)

    # Print the information.
    print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}")

    # Step the learning rate scheduler
    scheduler.step()

    # ---------- Early Stopping ----------
    if valid_acc > best_acc:
        best_acc = valid_acc
        epochs_no_improve = 0
        # Optionally save the best model
        # torch.save(model.state_dict(), 'best_model.pth')
    else:
        epochs_no_improve += 1
        if epochs_no_improve == patience:
            print(f"Early stopping after {epoch + 1} epochs.")
            break

"""## Testing

"""

model.eval()

predictions = []

for batch in tqdm(test_loader):
  # Fake labels should be created to make predictions work normally.
  imgs, labels = batch

  with torch.no_grad():
    logits = model(imgs.to(device))

  predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())

with open('predict.csv', 'w') as f:
  f.write('Id,Category\n')
  for i, pred in enumerate(predictions):
    f.write(f"{i},{pred}\n")
